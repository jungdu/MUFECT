# 4. 동영상 내보내기 원리 (Export & FFmpeg)

화면에서 실시간으로 움직이는 것을 보는 것과, 그것을 `mp4` 파일로 저장하는 것은 완전히 다른 문제입니다.

## 1. 실시간 녹화의 한계

보통 "화면 녹화" 기능을 생각하기 쉽지만, 웹 브라우저에서 고화질(1080p, 4K)으로 실시간 녹화를 하려고 하면 컴퓨터가 버벅거려서 프레임이 끊기거나 싱크가 안 맞는 문제가 발생합니다.

## 2. 프레임 바이 프레임 (Frame-by-Frame) 렌더링

그래서 우리는 **"시간을 멈추는 방식"**을 사용합니다. 실시간이 아니라, 아주 꼼꼼하게 한 장씩 그려서 저장하는 방식입니다.

### 핵심 기술: OfflineAudioContext
일반 오디오 컨텍스트는 실제 시간(Real-time)으로 흐르지만, `OfflineAudioContext`는 가상의 시간을 다룹니다.

1. **시간 정지**: 0.033초(1프레임) 단위로 시간을 딱 멈춥니다 (`suspend`).
2. **데이터 추출**: 멈춘 그 순간의 주파수 데이터를 뽑습니다.
3. **그리기 & 저장**: 그 데이터로 캔버스에 그림을 그리고, 이미지 파일(`frame00001.png`)로 저장합니다.
4. **시간 재개**: 다시 다음 프레임까지 시간을 흘려보냅니다 (`resume`).

이 과정을 노래가 끝날 때까지 수천 번 반복합니다. 사용자는 "렌더링 중..." 이라는 로딩 바를 보게 되지만, 결과물은 끊김 없이 완벽한 초고화질 영상을 얻을 수 있습니다.

## 3. FFmpeg의 역할 (조립하기)

위 과정이 끝나면 우리에게는 수천 장의 `png` 이미지 파일과 `mp3` 음악 파일 하나가 있습니다. 이것을 하나의 동영상으로 합쳐주는 도구가 **FFmpeg**입니다.

FFmpeg은 전 세계에서 가장 유명한 오픈소스 멀티미디어 처리 도구입니다. 보통은 서버나 PC에 설치해서 쓰지만, 우리는 **WebAssembly (WASM)** 기술을 이용해 브라우저 안에서 FFmpeg을 실행합니다 (`@ffmpeg/ffmpeg`).

### 실제 동작 명령어
우리는 FFmpeg에게 다음과 같은 명령을 내립니다:

```bash
ffmpeg -framerate 30 -i frame%05d.png -i input.mp3 -c:v libx264 -pix_fmt yuv420p output.mp4
```

- `-framerate 30`: 1초에 이미지 30장을 보여줘라.
- `-i frame%05d.png`: `frame00001.png`, `frame00002.png`... 순서대로 가져와라.
- `-i input.mp3`: 배경 음악을 넣어라.
- `-c:v libx264`: 비디오 코덱은 표준 H.264를 써라.
- `output.mp4`: 결과물 이름이다.

## 4. 요약

우리의 내보내기 과정은 **"사진관"**과 같습니다.
1. 음악을 아주 짧게 틉니다.
2. 멈춥니다.
3. 그 순간의 모습을 사진 찍어 저장합니다.
4. 다시 조금 틉니다.
5. 멈추고 찍습니다.
6. 마지막에 이 사진들을 빠르게 넘겨(Flipbook) 동영상으로 만듭니다.

이 방식 덕분에 컴퓨터 사양이 낮아도 시간은 좀 걸릴지언정 **절대 끊기지 않는 고품질 영상**을 만들 수 있습니다.
